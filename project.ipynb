{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOl9HvsJ+QYyfTp4R20YeYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gummadirajulavamshi/NLP/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxyxU-EH6pip",
        "outputId": "d0212a55-e38e-4d63-ef51-986731edb08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss 1.3876 acc 0.2430 | Val loss 1.3794 acc 0.2990\n",
            "Epoch 2: Train loss 1.3787 acc 0.3370 | Val loss 1.3664 acc 0.4410\n",
            "Epoch 3: Train loss 1.3498 acc 0.4030 | Val loss 1.2682 acc 0.4700\n",
            "Epoch 4: Train loss 1.1736 acc 0.4890 | Val loss 0.9561 acc 0.6870\n",
            "Epoch 5: Train loss 0.8078 acc 0.7370 | Val loss 0.4924 acc 0.8550\n",
            "Preds: [1, 2, 2, 1, 1, 1, 0, 3]\n",
            "True: [1, 2, 2, 1, 1, 1, 3, 3]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"General attention mechanism described for HAN.\n",
        "    Given encoder outputs H (batch, time, hidden), compute attention weighted sum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.context_vector = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, h: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        u = torch.tanh(self.proj(h))\n",
        "        scores = self.context_vector(u).squeeze(-1)\n",
        "        if mask is not None:\n",
        "\n",
        "            scores = scores.masked_fill(~mask, -1e9)\n",
        "\n",
        "        alpha = F.softmax(scores, dim=1)  # (batch, time)\n",
        "\n",
        "        alpha_unsq = alpha.unsqueeze(-1)  # (batch, time, 1)\n",
        "        attended = torch.sum(h * alpha_unsq, dim=1)  # (batch, hidden)\n",
        "        return attended, alpha\n",
        "\n",
        "\n",
        "class WordEncoder(nn.Module):\n",
        "    \"\"\"Encodes words within a sentence using Embedding + BiLSTM + Attention\n",
        "\n",
        "    Inputs expected per batch: sentences padded to max_words with word indices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, word_hidden_size: int, pretrained_embeddings: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "        self.bilstm = nn.LSTM(embed_dim, word_hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.attention = Attention(word_hidden_size * 2)\n",
        "\n",
        "    def forward(self, sentences: torch.Tensor, word_lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        embeds = self.embedding(sentences)\n",
        "\n",
        "\n",
        "        packed = pack_padded_sequence(embeds, word_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.bilstm(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (bs, max_time, hidden*2)\n",
        "\n",
        "\n",
        "        max_time = out.size(1)\n",
        "        mask = torch.arange(max_time, device=word_lengths.device).expand(len(word_lengths), max_time) < word_lengths.unsqueeze(1)\n",
        "\n",
        "\n",
        "        sentence_rep, word_alphas = self.attention(out, mask)\n",
        "        return sentence_rep, word_alphas\n",
        "\n",
        "\n",
        "class SentenceEncoder(nn.Module):\n",
        "    \"\"\"Encodes sentences within a document using BiLSTM + Attention\n",
        "    Input: sentence representations (batch, max_sents, sent_hidden)\n",
        "    Also takes sentence lengths (num valid sentences per document)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sent_hidden_size: int, doc_hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.bilstm = nn.LSTM(sent_hidden_size, doc_hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.attention = Attention(doc_hidden_size * 2)\n",
        "\n",
        "    def forward(self, sentences_rep: torch.Tensor, sent_lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        packed = pack_padded_sequence(sentences_rep, sent_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.bilstm(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (batch, max_sents, hidden*2)\n",
        "\n",
        "        max_sents = out.size(1)\n",
        "        mask = torch.arange(max_sents, device=sent_lengths.device).expand(len(sent_lengths), max_sents) < sent_lengths.unsqueeze(1)\n",
        "\n",
        "        doc_rep, sent_alphas = self.attention(out, mask)\n",
        "        return doc_rep, sent_alphas\n",
        "\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    \"\"\"Full Hierarchical Attention Network\n",
        "    Combines WordEncoder and SentenceEncoder and a final classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, word_hidden_size: int, sent_hidden_size: int, num_classes: int, pretrained_embeddings: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        self.word_encoder = WordEncoder(vocab_size, embed_dim, word_hidden_size, pretrained_embeddings)\n",
        "        self.sent_encoder = SentenceEncoder(word_hidden_size * 2, sent_hidden_size)\n",
        "        self.classifier = nn.Linear(sent_hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, docs: torch.Tensor, word_lengths: torch.Tensor, sent_lengths: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
        "\n",
        "        batch, max_sents, max_words = docs.size()\n",
        "\n",
        "\n",
        "        flat_sentences = docs.view(batch * max_sents, max_words)\n",
        "        flat_word_lengths = word_lengths.view(batch * max_sents)\n",
        "\n",
        "\n",
        "        nonzero_mask = flat_word_lengths > 0\n",
        "        filtered_sentences = flat_sentences[nonzero_mask]\n",
        "        filtered_word_lengths = flat_word_lengths[nonzero_mask]\n",
        "\n",
        "\n",
        "        word_encoder_output_dim = self.word_encoder.bilstm.hidden_size * 2\n",
        "        sentence_reps = torch.zeros(batch * max_sents, word_encoder_output_dim, device=docs.device)\n",
        "        word_alphas = torch.zeros(batch * max_sents, max_words, device=docs.device)\n",
        "\n",
        "        if filtered_sentences.size(0) > 0:\n",
        "            processed_sentence_reps, processed_word_alphas_raw = self.word_encoder(filtered_sentences, filtered_word_lengths)\n",
        "            sentence_reps[nonzero_mask] = processed_sentence_reps\n",
        "\n",
        "\n",
        "            current_alpha_len = processed_word_alphas_raw.size(1)\n",
        "            if current_alpha_len < max_words:\n",
        "\n",
        "                pad = torch.zeros(processed_word_alphas_raw.size(0), max_words - current_alpha_len, device=docs.device)\n",
        "                processed_word_alphas = torch.cat([processed_word_alphas_raw, pad], dim=1)\n",
        "            elif current_alpha_len > max_words:\n",
        "\n",
        "                processed_word_alphas = processed_word_alphas_raw[:, :max_words]\n",
        "            else:\n",
        "                processed_word_alphas = processed_word_alphas_raw\n",
        "\n",
        "            word_alphas[nonzero_mask] = processed_word_alphas\n",
        "\n",
        "\n",
        "\n",
        "        sentence_reps = sentence_reps.view(batch, max_sents, -1)  # (batch, max_sents, sent_hidden)\n",
        "        word_alphas = word_alphas.view(batch, max_sents, -1) # reshape word_alphas for output\n",
        "\n",
        "\n",
        "        doc_rep, sent_alphas = self.sent_encoder(sentence_reps, sent_lengths)\n",
        "\n",
        "        logits = self.classifier(doc_rep)\n",
        "        return logits, {\"word_alphas\": word_alphas, \"sent_alphas\": sent_alphas}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SyntheticDataset(Dataset):\n",
        "    \"\"\"Generates random \"documents\" for demonstration. Replace with real dataset.\n",
        "\n",
        "    Each document is represented as a tensor of shape (max_sents, max_words) containing token ids.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_docs: int, max_sents: int, max_words: int, vocab_size: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.num_docs = num_docs\n",
        "        self.max_sents = max_sents\n",
        "        self.max_words = max_words\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        import random\n",
        "        self.docs = []\n",
        "        self.labels = []\n",
        "        for _ in range(num_docs):\n",
        "            sents = random.randint(1, max_sents)\n",
        "            doc = []\n",
        "            word_counts = []\n",
        "            for _s in range(sents):\n",
        "                w = random.randint(1, max_words)\n",
        "                word_counts.append(w)\n",
        "                sent = [random.randint(1, vocab_size - 1) for _ in range(w)]\n",
        "\n",
        "                sent += [0] * (max_words - w)\n",
        "                doc.append(sent)\n",
        "\n",
        "            for _ in range(max_sents - sents):\n",
        "                doc.append([0] * max_words)\n",
        "                word_counts.append(0)\n",
        "            self.docs.append(doc)\n",
        "            self.labels.append(random.randint(0, num_classes - 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_docs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        doc = torch.tensor(self.docs[idx], dtype=torch.long)\n",
        "        word_lengths = (doc != 0).sum(dim=1).long()\n",
        "        sent_length = (word_lengths > 0).sum().long()\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return doc, word_lengths, sent_length, label\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    docs = torch.stack([item[0] for item in batch], dim=0)\n",
        "    word_lengths = torch.stack([item[1] for item in batch], dim=0)\n",
        "    sent_lengths = torch.stack([item[2] for item in batch], dim=0)\n",
        "    labels = torch.stack([item[3] for item in batch], dim=0)\n",
        "    return docs, word_lengths, sent_lengths, labels\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for docs, word_lengths, sent_lengths, labels in dataloader:\n",
        "        docs = docs.to(device)\n",
        "        word_lengths = word_lengths.to(device)\n",
        "        sent_lengths = sent_lengths.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch, max_sents, max_words = docs.size()\n",
        "        flat_word_lengths = word_lengths.view(batch * max_sents)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, alphas = model(docs, flat_word_lengths, sent_lengths)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        total += labels.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def eval_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for docs, word_lengths, sent_lengths, labels in dataloader:\n",
        "            docs = docs.to(device)\n",
        "            word_lengths = word_lengths.to(device)\n",
        "            sent_lengths = sent_lengths.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            batch, max_sents, max_words = docs.size()\n",
        "            flat_word_lengths = word_lengths.view(batch * max_sents)\n",
        "\n",
        "            logits, alphas = model(docs, flat_word_lengths, sent_lengths)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            total += labels.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    VOCAB_SIZE = 5000\n",
        "    EMBED_DIM = 100\n",
        "    WORD_HIDDEN = 50\n",
        "    SENT_HIDDEN = 50\n",
        "    NUM_CLASSES = 4\n",
        "\n",
        "    MAX_SENTS = 10\n",
        "    MAX_WORDS = 20\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = HAN(VOCAB_SIZE, EMBED_DIM, WORD_HIDDEN, SENT_HIDDEN, NUM_CLASSES).to(device)\n",
        "\n",
        "    dataset = SyntheticDataset(num_docs=1000, max_sents=MAX_SENTS, max_words=MAX_WORDS, vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, 6):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch {epoch}: Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
        "\n",
        "    docs, word_lengths, sent_lengths, labels = next(iter(val_loader))\n",
        "    docs = docs.to(device)\n",
        "    flat_word_lengths = word_lengths.view(docs.size(0) * docs.size(1)).to(device)\n",
        "    logits, alphas = model(docs, flat_word_lengths, sent_lengths.to(device))\n",
        "    preds = logits.argmax(dim=1)\n",
        "    print(\"Preds:\", preds[:8].tolist())\n",
        "    print(\"True:\", labels[:8].tolist())"
      ]
    }
  ]
}